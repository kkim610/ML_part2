{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e20f44c9-410c-497b-a5fc-984912037756",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Linear Models\n",
    "\n",
    "For classification and regression.\n",
    "\n",
    "Linear models are easy to understand and fast to train.\n",
    "They are typically good baselines.\n",
    "\n",
    "We will cover intuitions on how they work in a machine learning \n",
    "settings.\n",
    "\n",
    "<img src=\"../figures/scikit-learn-logo.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90577db-08e1-4937-96a9-4121697f92e0",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "* What is a linear model?\n",
    "* For regression: linear regression\n",
    "* For classification: logistic regression\n",
    "* Non linearly separable data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e67d64-1550-4449-97f7-cda79ed2b54f",
   "metadata": {},
   "source": [
    "# An example: estimating housing prices\n",
    "\n",
    "\n",
    "| Gr_Liv_Area | Year_Built | Full_Bath | Sale_Price |\n",
    "| ----------- | ---------- | --------- | ---------- |\n",
    "|        1656 |       1960 |         1 |      215.0 |\n",
    "|         896 |       1961 |         1 |      105.0 |\n",
    "|        1329 |       1958 |         1 |      172.0 |\n",
    "|        2110 |       1968 |         2 |      244.0 |\n",
    "|        1629 |       1997 |         2 |      189.9 |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b973d1f-fadc-46f4-a56b-8ba21871c187",
   "metadata": {},
   "source": [
    "### Linear approximation of Sale_Price:\n",
    "\n",
    "```\n",
    "Sale_Price =       0.1 * Gr_Liv_Area\n",
    "              +    1.1 * Year_Built\n",
    "              -    8.9 * Full_Bath\n",
    "              - 2200.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505ccdea-3521-4929-bb9e-ca9d498e2bd5",
   "metadata": {},
   "source": [
    "The goal is to estimate sale prices from numerical features characterizing\n",
    "recently sold houses in a given city. The target is continuous, so we are\n",
    "dealing with a regression problem instead of a classification problem.\n",
    "\n",
    "The linear model assumes that the sale price (here expressed in thousands of\n",
    "dollars) can be approximated by a linear combination of the features\n",
    "(explanatory variables) + a final offset (also known as the intercept).\n",
    "\n",
    "The learning procedure consists in estimating best possible values of the\n",
    "coefficients of the linear combinations to minimize the average prediction\n",
    "error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab20827-4885-4057-90b8-a13f59c72b25",
   "metadata": {},
   "source": [
    "Gr_Liv_Area: Ground Living Area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491a8429-6cc7-4c66-a4fb-2ff0add275b8",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "\n",
    "Predict the value of the target `y` given some observation `x`\n",
    "\n",
    "\n",
    "<img src=\"../figures/linear_data.svg\" width=\"40%\">\n",
    "\n",
    "\n",
    "\n",
    "### Linear approximation of Sale_Price:\n",
    "\n",
    "```\n",
    "Sale_Price =       0.1 * Gr_Liv_Area\n",
    "              - 2200.0\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99726ded-2fb1-4742-ab54-796f733bfbf5",
   "metadata": {},
   "source": [
    "For illustration purpose, let's consider a 1-dimensional observations:\n",
    "explaining the price as a function of a single feature, for instance the gross\n",
    "living area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31640d8-7155-41bb-8e2f-438f35f59aa2",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "\n",
    "Fit a prediction line as close as possible to all training points.\n",
    "\n",
    "\n",
    "<img src=\"../figures/linear_fit.svg\" width=\"40%\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "848dbcb3-2ded-4b01-9b9e-ec721c8d179c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n\u001b[0;32m      3\u001b[0m linear_regression \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[1;32m----> 4\u001b[0m linear_regression\u001b[38;5;241m.\u001b[39mfit(X, y)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b3b59e-c08a-4dbb-ad32-d78cd8cc0dc7",
   "metadata": {},
   "source": [
    "We learn a linear function to predict `y`. Here, the price is expressed\n",
    "as a constant multiplied by the area plus an intercept.\n",
    "\n",
    "Learning this function consists in finding the straight line which is\n",
    "as close as possible as all the data points. \n",
    "\n",
    "The corresponding model can then be used to make predictions for any\n",
    "possible `x`, as displayed by the blue line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50195a1c-2db1-4212-b07c-468f4b1a83ca",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "\n",
    "The slope is chosen to minimize the distance between the prediction and the\n",
    "data points\n",
    "\n",
    "<img src=\"../figures/linear_fit_red.svg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b4afb3-9068-4aa0-b70a-c52844af61bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435af32b-b3cd-4e33-a15a-bb5dd58af93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_pred = linear_regression.predict(X)\n",
    "squared_error = np.sum((y - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3223cd6c-dc1d-48b3-ac1a-7bf99475e178",
   "metadata": {},
   "source": [
    "The best fit is represented by the blue line which minimizes the sum of the\n",
    "square differences between the predicted values and the values of the target\n",
    "variable represented by the red segments.\n",
    "\n",
    "This minimization happens when we call the `fit` method of the\n",
    "`LinearRegression` class. The result is the automated tuning of the slope and\n",
    "intercept coefficient of the linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6471ffb-dc98-4c76-a0cc-bba709ba4a61",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Linear regression with several variables\n",
    "\n",
    "\n",
    "<img src=\"../figures/lin_reg_3D.svg\" width=\"40%\">\n",
    "\n",
    "\n",
    "\n",
    "The mental picture needs to be extended to several dimensions.\n",
    "\n",
    "For instance, in 2D:\n",
    "\n",
    "```\n",
    "Sale_Price =       0.1 * Gr_Liv_Area\n",
    "              +    1.1 * Year_Built\n",
    "              - 2209.0\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e53928e-15d1-49f7-8c57-d09f0e91797c",
   "metadata": {},
   "source": [
    "With more variables, the mental picture needs to be extended to several\n",
    "dimensions. However, the idea is the same: a linear model tries to\n",
    "minimize the error between the predictions and the data points.\n",
    "The predictions now form a plane.\n",
    "\n",
    "Often, the data have many features, and thus many dimensions. It is common to\n",
    "build models with hundreds of variables. It is no longer possible to visualize\n",
    "the fitting with a simple figure.\n",
    "\n",
    "For some applications in biology such as Genetics for instance practitioners\n",
    "use hundreds of thousands of input variables. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe98790-0209-4a83-8a1f-3c88704cacf2",
   "metadata": {},
   "source": [
    "# For classification: logistic regression\n",
    "\n",
    "For **classification**, we use a logistic regression model: `y` is either 0\n",
    "(blue) or 1 (red)\n",
    "\n",
    "\n",
    "<img src=\"../figures/categorical.svg\" width=\"40%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26159b7-be4d-455f-bf8f-2b06520a8e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c80ee1-abc6-4a8a-bca7-7ce61851d45c",
   "metadata": {},
   "source": [
    "The prediction target, `y`, is binary. It can be represented by either\n",
    "0 or 1. However, a straight line is not suited to try to explain\n",
    "such binary target.\n",
    "\n",
    "Hence, dedicated linear models for classification are needed. *Logistic\n",
    "regression* is such a model: it is intended for **classification** and\n",
    "not regression as the name would wrongly suggest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dede1c0-bd3a-4694-b5c2-acc448dc5e0a",
   "metadata": {},
   "source": [
    "# For classification: logistic regression\n",
    "\n",
    "The output of the model is interpreted as the probability of\n",
    "y being 1 (red) for a given x.\n",
    "\n",
    "\n",
    "<img src=\"../figures/logistic_color.svg\" width=\"40%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891f68a4-c257-457c-a17b-704dbdd44b96",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f585cb3c-ff7a-4183-9583-cb22c2ee9ff9",
   "metadata": {},
   "source": [
    "With logistic regression, the output is modeled using a form of soft\n",
    "step function, adjusted to the data. This function is called a logistic\n",
    "function. Using a soft, graduate shift between *y = 0* and *y = 1* is\n",
    "useful to capture the grey zone, where the value of *x* is not enough\n",
    "to decide whether the target value is 0 (blue) or 1 (red) with high\n",
    "confidence.\n",
    "\n",
    "In scikit-learn, this is done with the `LogisticRegression` object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52edf532-4985-4484-9d57-f8168308ce5f",
   "metadata": {},
   "source": [
    "\n",
    "# Logistic regression in 2 dimensions\n",
    "\n",
    "`X` is 2-dimensional, `y` is represented by the color\n",
    "\n",
    "\n",
    "<img src=\"../figures/logistic_2D.svg\" />\n",
    "\n",
    "<img src=\"../figures/logistic_3D.svg\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311e6bcb-fc26-487c-a381-54a576d43c5f",
   "metadata": {},
   "source": [
    "If the data has two features, it is convenient to represent it\n",
    "differently.\n",
    "\n",
    "Here, `X` has two dimensions `x1` and `x2`.\n",
    "\n",
    "The data points are represented as dots, the input features now appear as two\n",
    "dimensions that give the location of the data point in a 2D plane.\n",
    "The target is to predict the color of the data points that represent the class\n",
    "membership.\n",
    "\n",
    "2D surface that represent the probability to belong to the red class `y = 1` at\n",
    "a given location `x`. The decision function learned by logistic regression is\n",
    "represented by the soft any given position in the `(x1, x2)` space.\n",
    "\n",
    "This `(x1, x2)` space is called the feature space. Logistic regression is a\n",
    "models that internally computes a weighted sum of the values of the input\n",
    "features (similarly to linear regression). As a result, the shape of the\n",
    "decision boundary between regions assigned different classes is a straight\n",
    "line.\n",
    "\n",
    "The models assigns probability close to 0.5 for features close to the decision\n",
    "boundary. For points far away from the decision boundary, the model confidently\n",
    "predict values very close to 0 or 1 on each side.\n",
    "\n",
    "For common machine learning use cases, we typically have many more than\n",
    "two input features. Unfortunately it's not really possible to graphically\n",
    "represent the decision boundary of a logistic regression model in high\n",
    "dimensional space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10225e45-7d6d-4426-8355-dc7e3ffdc36d",
   "metadata": {},
   "source": [
    "# Logistic regression & multiclass classification\n",
    "\n",
    "\n",
    "  <img src=\"../figures/multinomial.svg\" width=\"40%\">\n",
    "\n",
    "- `y` in {0, 1, 2}\n",
    "- `y` in {blue, orange, green}\n",
    "\n",
    "For a given input ``x``:\n",
    "  - predict one probability per class\n",
    "  - probabilities sum to 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e30793d-249b-4e07-aa7b-dd6d63bbfb97",
   "metadata": {},
   "source": [
    "So far, we have considered the case where the output `y` is binary.\n",
    "When there is more than 2 classes to choose from.\n",
    "\n",
    "The `LogisticRegression` estimator can natural be extended to support\n",
    "multiclass classification.\n",
    "\n",
    "Instead of predicting one number which can be interpreted as the probability of\n",
    "`x` being assigned class 1, the model nows predicts 3 numbers: the\n",
    "probabilities of `x` being either assigned class 0 (blue), 1 (orange) or 2\n",
    "(green). Those three numbers must sum to 1.\n",
    "\n",
    "The \"true\" value of `y` is one of those 3 possibilities. The class labels are\n",
    "exclusive.\n",
    "\n",
    "Agains the trained models finds a way to define regions of the feature space,\n",
    "one for each class. The shape of the decision boundary are (segments) of\n",
    "straight lines because Logistic Regression internally builds a linear\n",
    "combination of the input features: it is a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214043b1-35ca-4e38-8179-5985a88cfb8a",
   "metadata": {},
   "source": [
    "\n",
    "# Linear models are not suited to all data\n",
    "\n",
    "\n",
    "<img src=\"../figures/lin_separable.svg\">\n",
    "\n",
    "*Almost* linearly separable\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b9430f-5b32-447c-a239-6bb88bf1b676",
   "metadata": {},
   "source": [
    "<img src=\"../figures/lin_not_separable.svg\">\n",
    "\n",
    "**Not** linearly separable →&nbsp;Underfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60abd3d-f60f-4059-b417-f62db05f0615",
   "metadata": {},
   "source": [
    "\n",
    "Linear models work well if the classes are (almost) linearly separable.\n",
    "\n",
    "However, sometimes, the best decision boundary to separate classes is not well\n",
    "approximated by a straight line.\n",
    "\n",
    "In such a situation, we can either use non-linear models, or perform\n",
    "transformations on the data, to engineer new features. We will cover these in\n",
    "a the following notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7b970c-e6ea-4e29-a7a1-c03d8d528603",
   "metadata": {},
   "source": [
    "# Take home messages on linear models\n",
    "]\n",
    "\n",
    "* Simple and fast baselines for:\n",
    " - **regression**: linear regression\n",
    " - **classification**: logistic regression\n",
    "\n",
    "--\n",
    "* Can underfit when: `n_features << n_samples`\n",
    "  →&nbsp;engineering new features can help!\n",
    "\n",
    "--\n",
    "* Hard to beat when `n_features` is large\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8306aefc-2627-493e-97d8-c9fdf984240d",
   "metadata": {},
   "source": [
    "To summarize on linear models:\n",
    "\n",
    "They form good baselines that can be easily understood. A later lesson\n",
    "will cover in details the intuitive interpretations of linear-model\n",
    "coefficients.\n",
    "\n",
    "Linear models are fast to train and fast to predict, and hence convenient to\n",
    "interactively design predict models but also more efficient to deploy in\n",
    "production at scale and on energy limited hardware (mobile phones).\n",
    "\n",
    "For regression, a good choice is typically to use a Ridge regression,\n",
    "which adds a simple regularization.\n",
    "\n",
    "For classification, a good choice is to use a logistic regression. The\n",
    "scikit-learn implementation is regularized by default.\n",
    "\n",
    "When linear models underfit (on non-linearly separable data), it is often\n",
    "possible to engineer new features to mitigate the problem. We will see\n",
    "examples of this strategy in the next exercises and notebooks.\n",
    "\n",
    "Linear models are particularly useful when the number of features is larger\n",
    "than the number of samples: more complex model can typically struggle more than\n",
    "linear models in this regime for no added improvement in predictive\n",
    "performance.\n",
    "\n",
    "However in high dimensions, we need to use regularized linear models which\n",
    "will introduce in the next presentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8f66ad-1c08-4568-b10a-d3474506b9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2086c4e7-7016-4b22-a450-10e3023928eb",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "For regression and classification.\n",
    "\n",
    "<img src=\"../figures/scikit-learn-logo.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e189dc-f2dd-4acd-9b3b-2df235c9d020",
   "metadata": {},
   "source": [
    "Decision tree are built as a set of rules for both\n",
    "classification and regression problems.\n",
    "\n",
    "These are the building blocks for more elaborate models such\n",
    "as _random forest_ and _gradient boosting trees_, as we will see."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1375190c-b981-4784-8b6a-26b7976c9fbc",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "- What is a decision tree?\n",
    "- Training trees for classification & regression\n",
    "- Impact of the tree depth on overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1640a533-5ca3-40f3-b4d5-d14e10eb7ac1",
   "metadata": {},
   "source": [
    "# What is a decision tree?\n",
    "\n",
    "First, let us develop a bit the intuitions on what is a decision tree,\n",
    "and how it can form the basis of classification and regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a7d812-0dac-4843-8716-05c49255cf2c",
   "metadata": {},
   "source": [
    "\n",
    "# A example of a decision tree\n",
    "\n",
    "<img src=\"../figures/tree_example.svg\" width=\"80%\">]\n",
    "\n",
    "A decision tree is a set of rules, combined in a hierarchical manner.\n",
    "\n",
    "In this example, if a new point has to be classified:\n",
    "\n",
    "- we will first check the age feature, if it is lower than 28.5, we shall classify it as \"low income\".\n",
    "\n",
    "- Otherwise, depending on the hours per week feature, we will classify it as low or high income.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a90b64-c296-47cf-a4b1-474d3b67a426",
   "metadata": {},
   "source": [
    "# Classification with a decision tree\n",
    "\n",
    "\n",
    "<img src=\"../figures/tree_blue_orange1.svg\" width=\"40%\">\n",
    "\n",
    "\n",
    "<img src=\"../figures/tree2D_1split.svg\" width=\"40%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96998d68-ee14-4414-8c02-cf3794cae5aa",
   "metadata": {},
   "source": [
    "On both sides of the threshold, each leaf node stores the number of training\n",
    "points of each classes that reached that node by following the decisions\n",
    "starting from the root noode.\n",
    "\n",
    "On both sides of the threshold, we store in the child node the **number of\n",
    "training points that reached that node for each possible value of `y`**.\n",
    "\n",
    "Whenever we add a new split node, the selected feature and the optimal\n",
    "threshold are tuned by maximizing the improvement in a quantity named entropy\n",
    "or gini index that measures how mixed are the classes in the nodes. The goal is\n",
    "to find splits such that both child nodes are as pure as possible\n",
    "\n",
    "For test data points, we start again from the root of the tree and the same\n",
    "thresholds is used to decide which branch to follow at each split node until\n",
    "the test sample reaches a leaf node.\n",
    "\n",
    "The tree predicts the fraction of training points of each class that reached\n",
    "that node, divided by the total number of training points that reached that node. Those values can be interpreted as **class assignment probabilities**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd52586-e4da-4c1f-b920-5d2d2342239c",
   "metadata": {},
   "source": [
    "# Classification with a decision tree\n",
    "\n",
    "\n",
    "<img src=\"../figures/tree_blue_orange2.svg\" width=\"40%\">\n",
    "\n",
    "<img src=\"../figures/tree2D_2split.svg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8f6e1d-eef6-45e7-87fc-333a8d0ae02d",
   "metadata": {},
   "source": [
    "We can incrementally expand any leaf to refine the decision function.\n",
    "At each step, the leaf focuses on a smaller subregion of the space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45767d8b-0710-418e-8db2-4e1b55759268",
   "metadata": {},
   "source": [
    "# Classification with a decision tree\n",
    "\n",
    "\n",
    "<img src=\"../figures/tree_blue_orange3.svg\" width=\"40%\">\n",
    "\n",
    "<img src=\"../figures/tree2D_3split.svg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b8ea65-da6c-4833-aec0-c9c5e97189b6",
   "metadata": {},
   "source": [
    "In this example, after two splits, we obtain pure leaves.\n",
    "i.e. in each leaf, there is only one class.\n",
    "The max depth here is equal to 2. We do not need to go deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e03f754-11b2-479f-b0b0-51b8592a08ea",
   "metadata": {},
   "source": [
    "# Regression with a decision tree\n",
    "\n",
    "<img src=\"../figures/tree_regression1.svg\" width=\"40%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1939d9a0-7800-46a0-b3fa-d37a5a53e0ee",
   "metadata": {},
   "source": [
    "Decision trees can also fit regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a43eed9-20d9-4522-aa13-7686db8600f1",
   "metadata": {},
   "source": [
    "Let's consider the following simple regression problem with a single feature\n",
    "as input on the `x` axis. The `y` axis represent the continuous value of\n",
    "the `y` variable.\n",
    "\n",
    "Each black point represents a training data points.\n",
    "\n",
    "Observe that it is not possible to use a straight line to approximate this\n",
    "dataset. One could therefore expect simple linear regression models to underfit\n",
    "on this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aac82c-da3c-4687-85cb-e5babbfa20e0",
   "metadata": {},
   "source": [
    "# Regression with a decision tree\n",
    "\n",
    "\n",
    "<img src=\"../figures/tree_regression_structure1.svg\" width=\"40%\">\n",
    "\n",
    "<img src=\"../figures/tree_regression2.svg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa62677-eb72-4b85-ac45-9dbaec512813",
   "metadata": {},
   "source": [
    "As for classification we choose one feature and threshold value is selected for\n",
    "a this feature to define the root of the tree (the first split node).\n",
    "\n",
    "On both sides of the threshold, we store in the child node the **average value\n",
    "of `y`** for each training point that reached that node.\n",
    "\n",
    "Whenever we add a new split node, the selected feature and the optimal\n",
    "threshold are tuned by maximizing the improvement in prediction error that\n",
    "would result by the introduction of the split.\n",
    "\n",
    "For test data points, we start again from the root of the tree and the same\n",
    "thresholds is used to decide which branch to follow at each step until the test\n",
    "sample reaches a leaf node.\n",
    "\n",
    "The tree predicts the numerical value stored in the selected leaf node.\n",
    "\n",
    "As a result the decision tree prediction function is always piece-wise\n",
    "constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaa50a8-aca5-4ba6-b884-242f9937bd5a",
   "metadata": {},
   "source": [
    "# Regression with a decision tree\n",
    "\n",
    "\n",
    "<img src=\"../figures/tree_regression_structure2.svg\" width=\"40%\">\n",
    "\n",
    "<img src=\"../figures/tree_regression3.svg\" width=\"40%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bea341e-73c1-4875-b049-e4aac39b1b08",
   "metadata": {},
   "source": [
    "At each iteration, one considers which leaf node node can be further replaced\n",
    "by a split node to refine the prediction function, therefore replacing it by a\n",
    "new split node and 2 new leaf nodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b916a52-40f3-44ac-b21b-ec9479c1120f",
   "metadata": {},
   "source": [
    "# Regression with a decision tree\n",
    "\n",
    "\n",
    "<img src=\"../figures/tree_regression_structure3.svg\" width=\"40%\">\n",
    "\n",
    "<img src=\"../figures/tree_regression4.svg\" width=\"40%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e55b2c6-20cc-4c67-8718-7915aec00fc1",
   "metadata": {},
   "source": [
    "The learning algorithms stops developing the trees either when there is only\n",
    "one data points per leaf node or when we have reached pre-defined tree size,\n",
    "expressed either as a maximum depth or maximum number of leaf nodes.\n",
    "\n",
    "Decision Trees are non-parametric models: the maximum size of the tree depends\n",
    "on the number of data points in the training set: it is not possible to develop\n",
    "a very deep decision tree on a training set with just a few training point.\n",
    "\n",
    "Choosing a maximum size can significantly impact the generalization performance\n",
    "of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b506c5d0-732a-4c4d-a21f-4df6997f220c",
   "metadata": {},
   "source": [
    "# Tree depth and overfitting\n",
    "\n",
    "How the maximum depth impacts generalization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa09dc48-81dd-4319-83f0-87f189b7192d",
   "metadata": {},
   "source": [
    "<img src=\"../figures/dt_underfit.svg\" width=\"40%\">\n",
    "\n",
    "Underfitting\n",
    "\n",
    "`max_depth` or `max_leaf_nodes` too&nbsp;small\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "<img src=\"../figures/dt_fit.svg\" width=\"40%\">\n",
    "\n",
    "Best trade-off <br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf22726-1e89-41be-b5a0-325d4708ca13",
   "metadata": {},
   "source": [
    "<img src=\"../figures/dt_overfit.svg\" width=\"40%\">\n",
    "\n",
    "Overfitting\n",
    "\n",
    "`max_depth` or `max_leaf_nodes` too&nbsp;large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bebf85c-34a3-4d9e-aa04-4868ba6c0f38",
   "metadata": {},
   "source": [
    "# Take home messages\n",
    "\n",
    "- Sequence of simple decision rules:\n",
    "\n",
    "  one feature and one threshold at a time\n",
    "\n",
    "\n",
    "\n",
    "- No scaling required for numerical features\n",
    "\n",
    "\n",
    "\n",
    "- `max_depth` controls the trade-off between underfitting and overfitting\n",
    "\n",
    "\n",
    "\n",
    "- Mostly useful as a building block for ensemble models\n",
    "  - Random Forests\n",
    "  - Gradient Boosting Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b53cbc-f343-4a82-b6c9-5d2e2c865f34",
   "metadata": {},
   "source": [
    "Tree-based models are very nice because they are invariant to rescaling of\n",
    "numerical features.\n",
    "\n",
    "The fact that there decision function is very \"axis-aligned\" (one feature with\n",
    "one threshold at a time) make them work very whell on tabular datasets where\n",
    "different columns have very different physical units.\n",
    "\n",
    "However there piecewise constant nature makes there prediction function\n",
    "non-smooth.\n",
    "\n",
    "Individual decision trees are therefore are almost never competitive on their\n",
    "own but they can become very strong models when combined in ensembles which can\n",
    "make the prediction function much smoother."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

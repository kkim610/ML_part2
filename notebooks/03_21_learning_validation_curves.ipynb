{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d080997e-1305-4bf5-97cc-dee13b2a81bd",
   "metadata": {},
   "source": [
    "# Comparing train and test errors\n",
    "\n",
    "Varying complexity: validation curves\n",
    "\n",
    "Varying the sample size: learning curves\n",
    "\n",
    "Goal: understand the overfitting / underfitting\n",
    "trade-off\n",
    "\n",
    "<img src=\"../figures/scikit-learn-logo.svg\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60f5e84-8242-4fb1-b914-da6f9fc163df",
   "metadata": {},
   "source": [
    "So, now that we understand that there is a tradeoff between underfitting and\n",
    "overfitting, the question is: from a practical standpoint, how do we know on\n",
    "which side of the balance our model is sitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f9efee-52be-4ea2-8e15-ef707e4ded57",
   "metadata": {},
   "source": [
    "# Train vs test error\n",
    "\n",
    "<img src=\"../figures/linear_splines_test.svg\" width=50%>\n",
    "\n",
    "<br>\n",
    "Measure:\n",
    "\n",
    "- errors on test data (generalization)\n",
    "\n",
    "- errors on the train data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6c5c1e-83c2-4492-a234-3e09d578cd4b",
   "metadata": {},
   "source": [
    "To probe the tradeoff between underfit and overfit, our central\n",
    "tool will be to measure both the generalization error, on unseen\n",
    "test data, and the error on the data used to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f73387-241f-4309-8ff3-5c1c6270e2bb",
   "metadata": {},
   "source": [
    "\n",
    "# Train vs test error: increasing complexity\n",
    "\n",
    "<img src=\"../figures/polynomial_overfit_test_1.svg\" width=50%><br><br><br><br>\n",
    "<img src=\"../figures/polynomial_validation_curve_1.svg\"\n",
    "width=\"50%\">]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f6b4f2-b274-4076-bd5c-c538526fd164",
   "metadata": {},
   "source": [
    "We can look at these errors while varying the model complexity.\n",
    "\n",
    "If we start with a very simple model, the training error is similar to\n",
    "the testing error: the model does not have enough capacity to capture\n",
    "noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0177d91-b68f-4665-940f-9762a4cd9bb7",
   "metadata": {},
   "source": [
    "# Train vs test error: increasing complexity\n",
    "\n",
    "<img src=\"../figures/polynomial_overfit_test_2.svg\" width=50%> <br><br>\n",
    "<img src=\"../figures/polynomial_validation_curve_2.svg\" width=\"50%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f273daf-f11f-4577-8a62-cc0c4892f1a5",
   "metadata": {},
   "source": [
    "As we increase model complexity, both the train and the test errors go\n",
    "down: the model explains well the data, but does not capture noise yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be703f9d-7608-43c6-8165-ec4ea727006d",
   "metadata": {},
   "source": [
    "# Train vs test error: increasing complexity\n",
    "\n",
    "<img src=\"../figures/polynomial_overfit_test_5.svg\" width=50%>] <br><br><br>\n",
    "<img src=\"../figures/polynomial_validation_curve_5.svg\" width=\"50%\">\n",
    "\n",
    "\n",
    "Even more complex models fit the training data better, but they start\n",
    "capturing noise in their model fit. As a result, their error on the test\n",
    "data is larger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271b8a17-e51c-4b27-9779-fcbe92bfaca0",
   "metadata": {},
   "source": [
    "# Train vs test error: increasing complexity\n",
    "\n",
    "<img src=\"../figures/polynomial_overfit_test_9.svg\" width=50%> <br><br><br>\n",
    "<img src=\"../figures/polynomial_validation_curve_15.svg\"\n",
    "width=\"50%\">]\n",
    "\n",
    "\n",
    "\n",
    "As we keep increasing the model complexity, the training error keeps going down,\n",
    "while the test increases sharply. The model is overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9265d4fc-0518-46c5-bb85-c3c5bdd8cd00",
   "metadata": {},
   "source": [
    "**The big picture is that models that are too simple have similar train and\n",
    "test error, while models that are too complex have a small train error\n",
    "but a very large test error. There is a sweet spot in the middle, and\n",
    "this is where good machine-learning models lie.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e041f4-de49-46bc-86f4-e3c43209084b",
   "metadata": {},
   "source": [
    "\n",
    "# Varying sample size\n",
    "\n",
    "Another useful way to look at the tradeoff between underfit and overfit\n",
    "is with varying sample size. Such an analysis is often known as a\n",
    "learning curve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f417a2c8-9c71-4ce4-b4cc-e857b35b7b1d",
   "metadata": {},
   "source": [
    "<img\n",
    "src=\"../figures/polynomial_overfit_ntrain_42.svg\"\n",
    "width=50%><br><br><br>\n",
    "<img\n",
    "src=\"../figures/polynomial_learning_curve_42.svg\"\n",
    "width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989b8a22-f608-4d1e-a5b6-0b3996474360",
   "metadata": {},
   "source": [
    "If we fit a polynomial of degree 9 on a small dataset, we will not have\n",
    "enough data and we will be in an overfitting situation.\n",
    "As a result the train error will be low, and the test error high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15caf58c-556d-4b20-adfe-d50890dbad9b",
   "metadata": {},
   "source": [
    "# Varying sample size\n",
    "<img\n",
    "src=\"../figures/polynomial_overfit_ntrain_145.svg\"\n",
    "width=50%><br><br><br>\n",
    "<img\n",
    "src=\"../figures/polynomial_learning_curve_145.svg\"\n",
    "width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838fa25b-b75f-448d-8d90-23a5e2dd99be",
   "metadata": {},
   "source": [
    "As we increase the sample size, the test error decreases while\n",
    "the train error increases: the model is overfitting less."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0781f6-ecef-4fc9-b7a8-1c190c9ab73d",
   "metadata": {},
   "source": [
    "# Varying sample size\n",
    "\n",
    "<img\n",
    "src=\"../figures/polynomial_overfit_ntrain_1179.svg\"\n",
    "width=50%><br><br><br>\n",
    "<img\n",
    "src=\"../figures/polynomial_learning_curve_1179.svg\"\n",
    "width=\"50%\">\n",
    "\n",
    "\n",
    "With enough data, the train and test errors converge: the model no\n",
    "longer overfits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27c8f99-b8a2-4753-acd2-05948cd34ceb",
   "metadata": {},
   "source": [
    "# Varying sample size\n",
    "\n",
    "<img\n",
    "src=\"../figures/polynomial_overfit_ntrain_6766.svg\"\n",
    "width=50%><br><br><br>\n",
    "<img\n",
    "src=\"../figures/polynomial_learning_curve_6766.svg\"\n",
    "width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9128fccd-b271-4f40-bc52-ea451c8176a0",
   "metadata": {},
   "source": [
    "After a while, we reach diminishing returns: the test and the train error\n",
    "are no longer changing: adding more data does not bring any benefit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe6af0c-5a06-4a35-870a-4f72097984fd",
   "metadata": {},
   "source": [
    "Try more complex models?\n",
    "\n",
    "\n",
    "Given that more data are not improving model prediction, it may be useful\n",
    "to try more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616467f9-ec21-4a7e-8a3d-b0020b216e55",
   "metadata": {},
   "source": [
    "# Bayes error rate\n",
    "\n",
    "<img\n",
    "src=\"../figures/polynomial_overfit_ntrain_6766.svg\"\n",
    "width=50%><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f134dff5-832b-4ff9-b206-7e3e82a7d70a",
   "metadata": {},
   "source": [
    "In general, when adding more data does not provide improvements,\n",
    "it is useful to try more complex models.\n",
    "\n",
    "However, in the specific case of our example, the data-generating process\n",
    "in a degree-9 polynomial. As a result, more complex models will not\n",
    "improve the prediction: the present model has already captured all the\n",
    "non-random link between _X_ and _y_.\n",
    "\n",
    "The corresponding prediction is imperfect, yet, we cannot do better: the\n",
    "prediction is limited by the intrinsic randomness of the link between X\n",
    "and y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901575e4-3d97-408b-a6d0-4f813040534e",
   "metadata": {},
   "source": [
    "# Model families\n",
    "\n",
    "\n",
    "\n",
    "- statistical model\n",
    "\n",
    "- data-generating process\n",
    "  \n",
    "\n",
    "\n",
    "The excellent performance that we just observed for degree-9 polynomials\n",
    "when there is plenty of data comes from the perfect match between the\n",
    "statistical model used to analyze the data and the data-generating process.\n",
    "So far, we have used polynomials for both of them, however in practice,\n",
    "given some data, we seldom know a simple form of model in which the data is drawn.\n",
    "\n",
    "For this reason, the choice of family of model is crucial.\n",
    "\n",
    "Note that some of the most important model families will be presented in details\n",
    "in the next modules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0261abc5-c119-4c9c-9afd-a6ab022ef63d",
   "metadata": {},
   "source": [
    "Some family names: linear models, decision trees, random forests, kernel machines, multi-layer perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d7b230-5d4b-4f4a-8140-5d0fe0158835",
   "metadata": {},
   "source": [
    "# Different model families\n",
    "<br><br>\n",
    "<img\n",
    "src=\"../figures/different_models_complex_4.svg\"\n",
    "width=50%><br><br><br>\n",
    "\n",
    "- Different \"inductive bias\" (주어진 모형이 학습 시에는 만나보지 않았던 상황에 대하여 정확한 예측을 하기 위해 사용하는 추가적인 가정)\n",
    "\n",
    "- Different notion of \"complexity\"\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "Different model families come with different forms of complexity and bias\n",
    "(which we call inductive bias).\n",
    "\n",
    "For instance, polynomial regressions tend to be smooth, and their\n",
    "complexity is controlled by the degree of the polynomials used to fit the\n",
    "data. On the other hand, decision trees are locally constant. Their\n",
    "complexity is controlled by the number of constant regions that they fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca5a086-5e5a-44f8-bafc-e9d98d6d6e04",
   "metadata": {},
   "source": [
    "# Different model families\n",
    "\n",
    "<img\n",
    "src=\"../figures/different_models_complex_4.svg\"\n",
    "width=50%><br><br><br>\n",
    "<img\n",
    "src=\"../figures/different_models_complex_16.svg\"\n",
    "width=50%>\n",
    "\n",
    "\n",
    "Varying model complexity for these different families of model explores\n",
    "different underfit and overfit behavior.\n",
    "\n",
    "In general, forcing a model towards a simpler fit is called\n",
    "\"regularization\". In scikit-learn, and in machine learning in general,\n",
    "almost every model comes with a specific regularization strategy. Part of\n",
    "machine-learning practice is to understand the inductive biases and\n",
    "the regularization strategy to choose the right model. We will cover this\n",
    "later, as we explore various models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7647638f-b3ae-45a6-8138-67771bbc821e",
   "metadata": {},
   "source": [
    "\n",
    "# summary\n",
    "\n",
    "\n",
    "\n",
    "Models **overfit**:\n",
    "\n",
    "\n",
    "\n",
    "- number of examples in the training set is too small\n",
    "- testing error is much bigger than training error\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8989ed7-f9a3-455e-b815-7c08a3118eb9",
   "metadata": {},
   "source": [
    "Models **underfit**:\n",
    "\n",
    "\n",
    "- models fail to capture the shape of the training set\n",
    "- even the training error is large\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4074d864-db67-43e1-9155-f3f28ba968aa",
   "metadata": {},
   "source": [
    "Different model families = different complexity & inductive bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f52fd24-1fc9-47e4-b30d-7fe6754322cb",
   "metadata": {},
   "source": [
    "\n",
    "Let me conclude this lesson by recapitulating the take home messages.\n",
    "\n",
    "When the models are too complex for the data at hand, they overfit.\n",
    "This means that they explain the data that they have seen too well as\n",
    "they capture noise, and thus do not generalize to new data.\n",
    "\n",
    "On the opposite, when models are too simple for the data at hand, they\n",
    "underfit. This means that they capture no noise, but their ability to\n",
    "generalize is then limited by their expressivity.\n",
    "\n",
    "Comparing train and test error, for instance varying model complexity, or\n",
    "amount of data, can reveal this tradeoff.\n",
    "\n",
    "The notion of complexity, or the bias imposed by a model, are both\n",
    "specific to a model family. We will later study different model families.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cdee4f-ae5c-463e-843c-30595a6834dd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

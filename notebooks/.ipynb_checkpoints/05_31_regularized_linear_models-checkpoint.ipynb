{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d5f00dc-296d-4af3-8e22-c65522ef01c9",
   "metadata": {},
   "source": [
    "# Regularized Linear Models\n",
    "\n",
    "How to avoid overfitting?\n",
    "\n",
    "<img src=\"../figures/scikit-learn-logo.svg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c133266-9936-4601-87e4-949c641cd26a",
   "metadata": {},
   "source": [
    "# Do linear models overfit?\n",
    "\n",
    "- Linear models are simpler than alternatives  → they tend to overfit less than alternatives\n",
    "\n",
    "- They even often underfit when:\n",
    "  - `n_features` is small (e.g. less than 10 features)\n",
    "  - the problem is not linearly separable\n",
    "\n",
    "But..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982083fb-ed7e-4e76-a929-683256211837",
   "metadata": {},
   "source": [
    "# Linear models can also overfit!\n",
    "\n",
    "Possible causes:\n",
    "\n",
    "- `n_samples << n_features`\n",
    "- Many uninformative features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a166589b-e383-47e2-97e7-a10ad9a580d3",
   "metadata": {},
   "source": [
    "Example for linear regression:\n",
    "\n",
    "```\n",
    "Sale_Price =      0.1 * Gr_Liv_Area\n",
    "             +    1.1 * Year_Built\n",
    "             -    8.9 * Full_Bath\n",
    "             +    2.5 * First_Owner_Is_Born_In_January\n",
    "             -    1.5 * First_Owner_Is_Born_In_February\n",
    "             ...\n",
    "             - 2200.0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0973af5e-3d17-4791-87c1-b608cb1ebfef",
   "metadata": {},
   "source": [
    "Fitting linear models with thousands of unrelated features can make linear\n",
    "models easily overfit any data.\n",
    "\n",
    "One solution would be to filter-out useless features:\n",
    "\n",
    "- it's somewhat possible using automated [feature selection methods](\n",
    "    https://scikit-learn.org/stable/modules/feature_selection.html)\n",
    "\n",
    "- but this is not always easy to tell if a given decision should be included or\n",
    "  not\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730da903-2037-49a1-abd0-50995a904765",
   "metadata": {},
   "source": [
    "# Regularization can reduce overfitting\n",
    "\n",
    "\n",
    "Unregularized regression:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model \n",
    "import LinearRegression\n",
    "\n",
    "model = LinearRegression().fit(X, y)\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47aa528-ca0e-4bd3-a6b4-b3d7495e748f",
   "metadata": {},
   "source": [
    "Ridge regression:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = Ridge(alpha=0.01).fit(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dd208d-df48-4bcf-8607-918fcccc385b",
   "metadata": {},
   "source": [
    "**Ridge regression** pulls the coefficients towards 0.\n",
    "\n",
    "Large `alpha` → more regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211994fd-21ba-4a1d-950e-5ed504146d13",
   "metadata": {},
   "source": [
    "**Recommendation**: always use `Ridge` with a carefully tuned `alpha`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2397261-d781-49ef-9917-4833cea4bc70",
   "metadata": {},
   "source": [
    "If we have too many input features with regards to the number of samples, the\n",
    "linear model can overfit: it assigns non-zero weights to features that are\n",
    "correlated with the target variable purely by chance on the training set and\n",
    "this correlation would not generalize to future test data.\n",
    "\n",
    "As described in a previous lecture, the problem with overfitting is that the\n",
    "model learns a decision function that is too sensitive to training set specitic\n",
    "details: here the non-zero associations to unrelated factors such as the birth\n",
    "date of the first owner of the house. As a consequence, the model generalizes\n",
    "poorly.\n",
    "\n",
    "The solution is to regularize the model: to foster less complex solutions. For\n",
    "this purpose, a linear model can be regularized by slightly biasing it to\n",
    "choose smaller weights for an almost similar fit. This forces the coefficients\n",
    "of the features to be very close to zero, unless they are really required to\n",
    "reduce a comparatively large fraction of the training error.\n",
    "\n",
    "The `Ridge` estimator does this in scikit-learn.\n",
    "\n",
    "This model comes with a complexity parameter that controls the amount of\n",
    "regularization. This parameter is named `alpha`. The larger the value of\n",
    "`alpha`, the greater the regularization, and thus the smaller the coefficients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28924551-a6e4-43f8-8c65-ae645068e5e1",
   "metadata": {},
   "source": [
    "# Regularization on a simple example\n",
    "\n",
    "<img src=\"../figures/linreg_noreg_0_nogrey.svg\" width=\"40%\">\n",
    "\n",
    "Small training set\n",
    "\n",
    "Fit a linear model without regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634a88ef-c9b3-42a0-bb5b-4259f351b21a",
   "metadata": {},
   "source": [
    "To build an intuition on Ridge regression (regularized linear regression), let\n",
    "us consider again a minimal case with a single input feature and a small\n",
    "training set with just a few data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4108b5f2-c69a-4ca2-9c4f-103dda1efda8",
   "metadata": {},
   "source": [
    "# Regularization on a simple example\n",
    "\n",
    "<img src=\"../figures/linreg_noreg_0.svg\" width=\"40%\">]\n",
    "\n",
    "Small training set\n",
    "\n",
    "Fit a linear model without regularization\n",
    "\n",
    "Training points sampled at random\n",
    "\n",
    "Can overfit if the data is noisy!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94dc127-673e-465b-87e5-dd73e6490c17",
   "metadata": {},
   "source": [
    "# Regularization on a simple example\n",
    "\n",
    "\n",
    "<img src=\"../figures/linreg_noreg_0.svg\" width=\"32%\">\n",
    "--\n",
    "<img src=\"../figures/linreg_noreg_1.svg\" width=\"32%\">\n",
    "--\n",
    "<img src=\"../figures/linreg_noreg_2.svg\" width=\"32%\">\n",
    "<img src=\"../figures/linreg_noreg_3.svg\" width=\"32%\">\n",
    "<img src=\"../figures/linreg_noreg_4.svg\" width=\"32%\">\n",
    "<img src=\"../figures/linreg_noreg_5.svg\" width=\"32%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42444f71-698a-4529-bec9-6a21ef8c8dd2",
   "metadata": {},
   "source": [
    "Let's observe how the slope of the model fitted without regularization could be\n",
    "impacted if our linear regression model was trained on alternative training\n",
    "sets (with the same size) sampled at random from the same data distribution.\n",
    "\n",
    "This re-sampling of the training set can be simulated when following the\n",
    "cross-validation procedure for instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296bda93-271e-41f6-b8c4-fae4987cca1d",
   "metadata": {},
   "source": [
    "\n",
    "# Bias-variance tradeoff in Ridge regression\n",
    "\n",
    "<img src=\"../figures/linreg_noreg_0.svg\" width=\"50%\">\n",
    "\n",
    "`LinearRegression` (no&nbsp;regularization)\n",
    "\n",
    "High variance, no bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab654907-6d09-47d4-b6a9-35ca4d2bdee0",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"../figures/ridge_0_withreg.svg\" width=\"50%\">]\n",
    "\n",
    "`Ridge` regression (regularized)\n",
    "\n",
    "Lower variance, but biased!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de034d2e-b418-4e2a-bf86-218945158297",
   "metadata": {},
   "source": [
    "# Bias-variance tradeoff in Ridge regression\n",
    "\n",
    "\n",
    "<img src=\"../figures/ridge_alpha_0.svg\" width=\"40%\">\n",
    "\n",
    "Too much variance\n",
    "\n",
    "`alpha` too small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8f6437-93ab-438a-aa6d-f378b546d0f2",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"../figures/ridge_alpha_50.svg\" width=\"40%\">\n",
    "\n",
    "Best tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa839d09-2bba-490b-adcd-063612e33c70",
   "metadata": {},
   "source": [
    "<img src=\"../figures/ridge_alpha_500.svg\" width=\"40%\">\n",
    "\n",
    "Too much bias\n",
    "\n",
    "`alpha` too large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a254a1-1c15-4c7f-bf8c-e3cfc9c1db31",
   "metadata": {},
   "source": [
    "This is a typical example of bias/variance tradeoff: non-regularized\n",
    "estimators are not biased, but they can display a lot of variance.\n",
    "Highly-regularized models have little variance, but high bias.\n",
    "\n",
    "This bias is not necessarily a bad thing: what matters is choosing the\n",
    "tradeoff between bias and variance that leads to the best prediction\n",
    "performance. For a specific dataset there is a sweet spot corresponding\n",
    "to the highest complexity that the data can support, depending on the\n",
    "amount of noise and observations available.\n",
    "\n",
    "Given new data points, beyond our two initial measures, the sweep spot\n",
    "minimizes the error. For the specific case of the `Ridge` estimator, in\n",
    "scikit-learn, the best value of `alpha` can be automatically found\n",
    "using the `RidgeCV` object.\n",
    "\n",
    "Note that, in general, for prediction, it is always better to prefer\n",
    "`Ridge` over a `LinearRegression` object. Using at least a small amount\n",
    "of regularization is always useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a9b0b2-7b6a-49c9-b954-d7dd243181c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Automated tuning for regularization\n",
    "\n",
    "\n",
    " ```python\n",
    "from sklearn.linear_model \n",
    "import Ridge\n",
    "from sklearn.model_selection \n",
    "import GridSearchCV\n",
    "\n",
    "\n",
    "param_grid = {\"alpha\": [0.001, 0.1, 1, 10, 1000],}\n",
    "model = GridSearchCV(Ridge(), param_grid)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(model.best_parameters_)\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebf8de4-b807-4dc5-9767-33072d737682",
   "metadata": {},
   "source": [
    " ```python\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "\n",
    "model = RidgeCV(alphas=[0.001, 0.1, 1, 10, 1000])\n",
    "model.fit(X, y)\n",
    "\n",
    "print(model.alpha_)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d114bc10-024b-422c-b2e6-ac14415460e1",
   "metadata": {},
   "source": [
    "It is possible to use cross-validation and the grid search procedure to tune\n",
    "the value of alpha for a given problem.\n",
    "\n",
    "But for ridge regression it's also possible to use the `RidgeCV` class that can\n",
    "run a very efficient internal tuning procedure that can be significantly faster\n",
    "than running a traditional grid search.\n",
    "\n",
    "The selected value for the parameter `alpha` is stored as the attribute\n",
    "`model.alpha_` after calling fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f3d10d-7a89-40b1-80db-64910724d06e",
   "metadata": {},
   "source": [
    "# Regularization in logistic regression\n",
    "\n",
    "`LogisticRegression(C=1)` is regularized by default!\n",
    "\n",
    "High&nbsp;C&nbsp;value&nbsp;→&nbsp;weaker regularization.\n",
    "\n",
    "<img src=\"../figures/logistic_2D_C0.001.svg\" width=\"40%\">\n",
    "&nbsp;&nbsp;Small `C`\n",
    "\n",
    "<img src=\"../figures/logistic_2D_C1.svg\" width=\"40%\">\n",
    "&nbsp;&nbsp;Large `C`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81c7d19-9529-4a8b-8440-95074330dcc8",
   "metadata": {},
   "source": [
    "For classification, logistic regression also comes with regularization.\n",
    "\n",
    "\n",
    "In scikit-learn, this regularization is controlled by a parameter called\n",
    "`C`, which has a slightly different behavior than `alpha` in the Ridge\n",
    "estimator.\n",
    "\n",
    "For a large value of `C`, the model puts more emphasis on the data points\n",
    "close to the frontier.\n",
    "On the contrary, for a low value of `C`, the model considers all the points.\n",
    "\n",
    "As with Ridge, the tradeoff controlled by the choice of `C` depends on\n",
    "the dataset and should be tuned for each set. This tuning can be done in\n",
    "scikit-learn using the `LogisticRegressionCV` object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6169a1d1-bd61-4bf0-9a2d-e83cdebdd275",
   "metadata": {},
   "source": [
    "# Take home messages on linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d55f62-45b6-436d-961f-b113e1867473",
   "metadata": {},
   "source": [
    "* Can overfit when:\n",
    "\n",
    " - `n_samples` is too small and `n_features` is large\n",
    " -  In particular with non-informative features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efafd68-5cc5-4684-a796-2fdafb42ef15",
   "metadata": {},
   "source": [
    "* Regularization for **regression**:\n",
    " - linear regression → ridge regression\n",
    " - large `alpha` parameter → strong regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a871596f-4c92-4477-9667-794cb18d2e88",
   "metadata": {},
   "source": [
    "* Regularization **classification**:\n",
    " - logistic regression regularized by default\n",
    " - small `C` parameter → strong regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017975a6-cb9e-40e4-8556-ba11df3f6619",
   "metadata": {},
   "source": [
    "Always use regularization when fitting linear models: you can tune the\n",
    "regularization parameter using cross-validation.\n",
    "\n",
    "In particular for Ridge regression `RidgeCV` is efficient fast at tuning\n",
    "`alpha` automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d75df7-5d6a-4b47-a0df-572098ea398f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

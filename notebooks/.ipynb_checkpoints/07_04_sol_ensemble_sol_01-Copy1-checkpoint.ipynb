{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dfbac0a",
   "metadata": {},
   "source": [
    "# ðŸ“ƒ Solution for Exercise M6.01\n",
    "\n",
    "The aim of this notebook is to investigate if we can tune the hyperparameters\n",
    "of a bagging regressor and evaluate the gain obtained.\n",
    "\n",
    "We will load the California housing dataset and split it into a training and\n",
    "a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c948895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data, target = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "target *= 100  # rescale the target in k$\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data, target, random_state=0, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011ee658-742c-467e-98f8-553c87b00852",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e155674",
   "metadata": {},
   "source": [
    "<div class=\"admonition note alert alert-info\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Note</p>\n",
    "<p class=\"last\">If you want a deeper overview regarding this dataset, you can refer to the\n",
    "Appendix - Datasets description section at the end of this MOOC.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4043f8",
   "metadata": {},
   "source": [
    "Create a `BaggingRegressor` and provide a `DecisionTreeRegressor`\n",
    "to its parameter `base_estimator`. Train the regressor and evaluate its\n",
    "generalization performance on the testing set using the mean absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078695fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor()\n",
    "bagging = BaggingRegressor(base_estimator=tree, n_jobs=2)\n",
    "bagging.fit(data_train, target_train)\n",
    "target_predicted = bagging.predict(data_test)\n",
    "print(f\"Basic mean absolute error of the bagging regressor:\\n\"\n",
    "      f\"{mean_absolute_error(target_test, target_predicted):.2f} k$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c814a572",
   "metadata": {},
   "source": [
    "Now, create a `RandomizedSearchCV` instance using the previous model and\n",
    "tune the important parameters of the bagging regressor. Find the best\n",
    "parameters  and check if you are able to find a set of parameters that\n",
    "improve the default regressor still using the mean absolute error as a\n",
    "metric.\n",
    "\n",
    "<div class=\"admonition tip alert alert-warning\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Tip</p>\n",
    "<p class=\"last\">You can list the bagging regressor's parameters using the <tt class=\"docutils literal\">get_params</tt>\n",
    "method.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8572c87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "for param in bagging.get_params().keys():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a325b0",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": randint(10, 30),\n",
    "    \"max_samples\": [0.5, 0.8, 1.0],\n",
    "    \"max_features\": [0.5, 0.8, 1.0],\n",
    "    \"base_estimator__max_depth\": randint(3, 10),\n",
    "}\n",
    "search = RandomizedSearchCV(\n",
    "    bagging, param_grid, n_iter=20, scoring=\"neg_mean_absolute_error\"\n",
    ")\n",
    "_ = search.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5614ef-885f-4e96-88d4-604e704b6b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomizedSearchCV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7296ce93",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = [f\"param_{name}\" for name in param_grid.keys()]\n",
    "columns += [\"mean_test_error\", \"std_test_error\"]\n",
    "cv_results = pd.DataFrame(search.cv_results_)\n",
    "cv_results[\"mean_test_error\"] = -cv_results[\"mean_test_score\"]\n",
    "cv_results[\"std_test_error\"] = cv_results[\"std_test_score\"]\n",
    "cv_results[columns].sort_values(by=\"mean_test_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f8856a",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "target_predicted = search.predict(data_test)\n",
    "print(f\"Mean absolute error after tuning of the bagging regressor:\\n\"\n",
    "      f\"{mean_absolute_error(target_test, target_predicted):.2f} k$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90fb4a0-1226-4033-8a29-18b2205801c5",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [10,30,1],\n",
    "    \"max_samples\": [0.5, 0.8, 1.0],\n",
    "    \"max_features\": [0.5, 0.8, 1.0],\n",
    "    \"base_estimator__max_depth\": [3, 10,1],\n",
    "}\n",
    "search = GridSearchCV(\n",
    "    bagging, param_grid, scoring=\"neg_mean_absolute_error\"\n",
    ")\n",
    "_ = search.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51ae4a3-74e4-4978-9957-c5794c42ead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = search.score(data_test, target_test)\n",
    "print(\n",
    "    f\"The test accuracy score of the grid-searched pipeline is: \"\n",
    "    f\"{accuracy:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62420ad7-793a-4b47-ae80-2e67a6473d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The best set of parameters is: \"\n",
    "      f\"{search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1c8beb-f673-4d72-8007-447b7dc94ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a415c5a-670e-4a7c-b1d0-8cc58b78b3d0",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = [f\"param_{name}\" for name in param_grid.keys()]\n",
    "columns += [\"mean_test_error\", \"std_test_error\"]\n",
    "cv_results = pd.DataFrame(search.cv_results_)\n",
    "cv_results[\"mean_test_error\"] = -cv_results[\"mean_test_score\"]\n",
    "cv_results[\"std_test_error\"] = cv_results[\"std_test_score\"]\n",
    "cv_results[columns].sort_values(by=\"mean_test_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d01d21-7628-4394-8d78-c644eb78604d",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "target_predicted = search.predict(data_test)\n",
    "print(f\"Mean absolute error after tuning of the bagging regressor:\\n\"\n",
    "      f\"{mean_absolute_error(target_test, target_predicted):.2f} k$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017423df",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "We see that the predictor provided by the bagging regressor does not need\n",
    "much hyperparameter tuning compared to a single decision tree."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
